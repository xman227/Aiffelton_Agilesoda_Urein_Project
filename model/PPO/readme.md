# PPO 모델 설계
- [참조](https://github.com/nikhilbarhate99/PPO-PyTorch)
- 환경에 맞춰서 구동하는데 초점을 주고 만들었습니다.
- 아직 어떤 오류 혹은 훈련이 안되는 상황이 있을지 불확실 하기 때문에 40000번의 훈련 이후 모델을 좀 더 최적화 혹은 수정할 예정입니다.
- 멘토님들의 피드백중 현재 환경에서는 GPU 연산이 적을 것이라는 얘기를 들어서 일단 GPU 없이 설계했습니다.
---
## 0525
- 설정을 잘못한게 이유일 수도 있지만 다른 모델보다 엄청나게 훈련 속도가 빠르다.
- 평균 보상이 점차 늘어나는 모습을 가시적으로 볼 수 있어서 아직까진 문제 없다 느껴진다.
---
## 0530
- 훈련이 잘되다가 어느순간부터 갑작스럽게 agent가 시작도 안하는 쪽으로 학습을 하기 시작했다.
### 학습 방향
---
  + 보상을 좀 더 키워서 훈련
  + 다시 한번 스텝별 보상 가중치 주기
  + 장애물에 부딪힐때 보상을 더 작게 
