{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462c12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1 -2 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zlxlekta924/anaconda3/envs/test/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Sim import *\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9943e9",
   "metadata": {},
   "source": [
    "\n",
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e726e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Architecture of Layers\n",
    "\n",
    "Dqn 에서의 구조\n",
    "84x84x4 사이즈\n",
    "순서대로 input, channel, kernel, strdides\n",
    "Conv2d(x, 32, 8, 4)\n",
    "conv2d(x, 64, 4, 2)\n",
    "conv2d(x, 64, 3, 1)\n",
    "flatten()\n",
    "dense(x, 512)\n",
    "dense(x, 2)\n",
    "\n",
    "이 때 우리는\n",
    "9x10x1 이고 약 300배 차이가 나므로\n",
    "\n",
    "Conv2d(x, 16, 4, 2)\n",
    "conv2d(x, 32, 3, 1)\n",
    "flatten()\n",
    "dense(x, 128)\n",
    "dense(x, 4)\n",
    "\n",
    "로 변경하면 괜찮을 듯\n",
    "'''\n",
    "\n",
    "class QnetCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QnetCNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=4, stride=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU())\n",
    "    \n",
    "        # Flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 , 32)\n",
    "        self.fc2 = nn.Linear(32, 4)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = torch.reshape(x, (-1, 1, 10, 9)).to(device)\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)# 4개의 값을 반환\n",
    "        return x\n",
    "\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) \n",
    "        coin = random.random() \n",
    "        \n",
    "\n",
    "        if coin < epsilon:  # 동전던지기, eps 확률로 랜덤하게 행동\n",
    "            return random.randint(0,3)\n",
    "        else:\n",
    "            return out.argmax().item() #Action state 값이 더 높은 index 값 호출\n",
    "\n",
    "\n",
    "def train(q, q_target, memory, optimizer): \n",
    "    for i in range(10):\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size) #32개를 버퍼에서 뽑아 모아 놓은 s,a,r,s_prime,done_mask\n",
    "\n",
    "        q_out = q(s)                                   # s 값으로 다음 각 action 값들의 value 값 반환\n",
    "        q_a = q_out.gather(1,a.to(device))         #선택한 액션값들의 q(s,a) 반환\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1) # 다음 state의 각 q(s,a) 값 반환\n",
    "        target = torch.tensor(r, device=device) + torch.tensor(gamma, device=device) * max_q_prime * torch.tensor(done_mask, device=device) # 배열 맞춰주기, 쓰러진 경우는 제거\n",
    "        loss = F.smooth_l1_loss(q_a, target) # DQN 의 손실함수 계산 L1 유클리드\n",
    "\n",
    "        optimizer.zero_grad() # optimizer 의 모든 parameter 를 0으로 변환\n",
    "        loss.backward() # loss 에 대한 gradient 계산\n",
    "        optimizer.step() # 손실값을 바탕으로 Qnet 의 파라미터 업데이트\n",
    "\n",
    "def model_save(model_dict, opt_dict, epi):\n",
    "    PATH = './weights/'\n",
    "    torch.save({\n",
    "            'model': model_dict,\n",
    "            'optimizer': opt_dict,\n",
    "            'epi': epi\n",
    "            }, PATH + 'all.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a993c6",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b6b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch. float), torch.tensor(a_lst), torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch. float), torch.tensor(done_mask_lst)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1709615",
   "metadata": {},
   "source": [
    "## Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f00e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                      | 13/400 [00:02<00:54,  7.11it/s]/home/zlxlekta924/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755861072/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "/home/zlxlekta924/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      " 13%|█████████▍                                                               | 52/400 [00:09<01:06,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 에서 챙기는 아이템 : ['B', 'G', 'H', 'J', 'L', 'N']\n",
      "마지막 행동 : ↑\n",
      " 그 때의 마지막 상태 :\n",
      "[[0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  0.3]\n",
      " [0.  1.  1.  1.  1.  1.  1.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.6 1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [1.  1.  0.  1.  0.  1.  0.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0. ]]\n",
      "마지막 보상 : -2\n",
      " 총 받은 보상 :-542\n",
      "score = -73, n_buffer : 15300, eps : 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████▎                                                     | 102/400 [00:18<00:56,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 에서 챙기는 아이템 : ['C', 'D', 'I', 'M', 'Q']\n",
      "마지막 행동 : ←\n",
      " 그 때의 마지막 상태 :\n",
      "[[0.  0.  0.  0.  0.6 0.  0.  0.  0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.3 1.  1.  1.  1.  1.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [1.  1.  0.  1.  0.  1.  0.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0. ]]\n",
      "마지막 보상 : -2\n",
      " 총 받은 보상 :1482\n",
      "score = -177, n_buffer : 30300, eps : 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████▎                                            | 152/400 [00:28<00:47,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 에서 챙기는 아이템 : ['H', 'I']\n",
      "마지막 행동 : ←\n",
      " 그 때의 마지막 상태 :\n",
      "[[0.  0.  0.  0.6 0.  0.  0.  0.  0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  1.  1.  1.  1.  1.  1.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [1.  1.  0.  1.  0.  1.  0.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.  0.  0.  0.3 0.  0.  0.  0. ]]\n",
      "마지막 보상 : -2\n",
      " 총 받은 보상 :-492\n",
      "score = -380, n_buffer : 45300, eps : 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████▎                                   | 202/400 [00:37<00:37,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 에서 챙기는 아이템 : ['A', 'F', 'H', 'I', 'K', 'N']\n",
      "마지막 행동 : ←\n",
      " 그 때의 마지막 상태 :\n",
      "[[0.  0.6 0.  0.  0.  0.  0.  0.  0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  1.  1.  1.  1.  1.  1.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  0.3 0.  1.  0.  1.  0. ]\n",
      " [1.  1.  0.  1.  0.  1.  0.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0. ]]\n",
      "마지막 보상 : -2\n",
      " 총 받은 보상 :606\n",
      "score = -218, n_buffer : 50000, eps : 36.00000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████▎                          | 252/400 [00:47<00:28,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 에서 챙기는 아이템 : ['B', 'C', 'E', 'L', 'N', 'P']\n",
      "마지막 행동 : ↓\n",
      " 그 때의 마지막 상태 :\n",
      "[[0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  1.  1.  1.  1.  1.  1.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.6 1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [1.  1.  0.  1.  0.  1.  0.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  0.3]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0. ]]\n",
      "마지막 보상 : -2\n",
      " 총 받은 보상 :-517\n",
      "score = -22, n_buffer : 50000, eps : 36.00000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████▎                 | 302/400 [00:56<00:18,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 에서 챙기는 아이템 : ['B', 'C', 'E', 'G', 'M', 'O', 'P']\n",
      "마지막 행동 : ←\n",
      " 그 때의 마지막 상태 :\n",
      "[[0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  1.  1.  1.  1.  1.  1.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.6 1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [1.  1.  0.  1.  0.  1.  0.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.3 1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0. ]]\n",
      "마지막 보상 : -2\n",
      " 총 받은 보상 :-503\n",
      "score = -57, n_buffer : 50000, eps : 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████▎        | 352/400 [01:06<00:09,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 에서 챙기는 아이템 : ['A', 'C', 'F', 'G', 'N', 'P', 'Q']\n",
      "마지막 행동 : →\n",
      " 그 때의 마지막 상태 :\n",
      "[[0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  1.  1.  1.  1.  1.  1.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.  1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [0.6 1.  0.  1.  0.  1.  0.  1.  0. ]\n",
      " [1.  1.  0.  1.  0.  1.  0.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  0.3 1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0. ]]\n",
      "마지막 보상 : -1\n",
      " 총 받은 보상 :-508\n",
      "score = -266, n_buffer : 50000, eps : 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 400/400 [01:16<00:00,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from Sim import *\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.99\n",
    "buffer_limit  = 50000\n",
    "batch_size = 32\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "\n",
    "env = Simulator()\n",
    "\n",
    "memory = ReplayBuffer()\n",
    "q = QnetCNN().to(device)\n",
    "q_target = QnetCNN().to(device)\n",
    "q_target.load_state_dict(q.state_dict()) # 현재 Qnet 의 파라미터를 q_target 에 load\n",
    "\n",
    "optimizer = optim.Adam(q.parameters(), lr = learning_rate) # loss 값을 바탕으로 업데이트할 비율 (q_target 말고 q 만 업데이트)\n",
    "\n",
    "print_interval = 50\n",
    "score = 0.0\n",
    "\n",
    "\n",
    "files = pd.read_csv(\"./data/factory_order_train.csv\")\n",
    "    \n",
    "    \n",
    "for epi in tqdm(range(400)): \n",
    "\n",
    "    if epi%100 == 0:\n",
    "        model_save(q.state_dict(), optimizer.state_dict(), epi)\n",
    "        \n",
    "    items = list(files.iloc[epi])[0]\n",
    "    \n",
    "    epsilon = max(0.01, 0.40 - 0.02 * (epi//100))\n",
    "\n",
    "    s = env.reset(epi)\n",
    "    obs = np.asarray(s, dtype=np.float32)\n",
    "    \n",
    "    done = False\n",
    "    first = True\n",
    "    a_step = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Action choose\n",
    "        if first:\n",
    "            a = 0\n",
    "            first = False\n",
    "        \n",
    "        else:\n",
    "            a = q.sample_action(torch.from_numpy(obs).float(), epsilon) \n",
    "            \n",
    "        s_prime, r, cumul, done, goal_reward = env.step(a)\n",
    "\n",
    "        array = s_prime\n",
    "        action = ['↑', '↓', '←', '→'][a]\n",
    "        s_prime = np.asarray(s_prime, dtype=np.float32)\n",
    "        obs = s_prime\n",
    "        \n",
    "        # Calculate\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((obs,a,r, s_prime, done_mask))\n",
    "        \n",
    "        score += r\n",
    "        a_step +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if a_step == 300 : #1000번 했는데도 안되면 강제종료\n",
    "            done =True\n",
    "        \n",
    "        if done :\n",
    "            break\n",
    "            \n",
    "    if memory.size() > 4000:\n",
    "        train(q, q_target, memory, optimizer)\n",
    "        \n",
    "    if epi%print_interval==0 and epi != 0: \n",
    "        print(f'{epi} 에서 챙기는 아이템 : {items}\\n마지막 행동 : {action}\\n 그 때의 마지막 상태 :\\n{array}\\n마지막 보상 : {r}\\n 총 받은 보상 :{cumul}')\n",
    "        q_target.load_state_dict(q.state_dict()) #q_target 지금걸로 업데이트\n",
    "        print(f\"score = {int(score/print_interval)}, n_buffer : {memory.size()}, eps : {epsilon*100}\")\n",
    "        score = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3377d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
